# POC_Data_Engineer


# Data Engineer POC - ETL Workflow

# Overview
This project demonstrates an end-to-end ETL workflow where Terraform is used to provision AWS infrastructure including an S3 bucket for source data and output results, Glue database and tables using crawlers, and IAM roles. A Glue Python job is defined to read source CSV data via Athena, calculate lag time between user events, and write the results back to S3 in CSV format.

# How It Works
This ETL pipeline is designed to calculate the lag time between user events by orchestrating a sequence of AWS services, all provisioned using Terraform for consistency and scalability. Here's a breakdown of how the workflow functions from ingestion to transformation and cataloging:
# 1.	Source Data Upload
The process begins when a CSV file containing user event data (e.g., login, click, logout) is uploaded to a dedicated source S3 bucket. This bucket is provisioned via Terraform and organized with folders for both incoming data and archived files.
# 2.	Source Crawler Execution
A Glue crawler, configured and scheduled within a Glue Workflow, scans the uploaded data in the source bucket. It creates or updates the metadata table in the Glue Data Catalog, allowing Athena to query the data in a structured format.
# 3.	Athena Query-Based Glue Job
The Glue ETL job is implemented in Python using PySpark and AWS SDK (boto3) within the Glue environment. Key highlights of the script include:
#Athena Integration: 
The script triggers an Athena query to read the source CSV data from the Glue catalog table, waiting for query completion before proceeding.
# Data Cleaning & Preprocessing: 
It selects required columns (user_id, event_time), trims string fields, converts timestamps to proper format, removes nulls and duplicates to ensure clean input data.
#Lag Time Calculation: 
Uses PySpark window functions to calculate the time lag in minutes between consecutive user events (lag() over user_id partition ordered by event time).
# Output Writing: 
The transformed data with lag time is written back to the specified target S3 path as a single CSV file with headers.
# Archival Logic: 
After successful processing, the script moves the source input files to an archive folder within the same S3 bucket to keep the input directory clean for future ingestions. This is done using the AWS boto3 client to copy and delete files programmatically.
# Logging and Error Handling: 
Detailed logging is included at each major step for monitoring and troubleshooting. Any failures result in a job exit with error logs.
# 4.	Target Crawler Execution
Once the ETL job completes, a second Glue crawler scans the output location. This updates the Glue Catalog with the transformed data, making it queryable for downstream analysis or reporting.
# 5.	Workflow Automation
All these steps are tied together using an AWS Glue Workflow:
•	It starts with the source crawler.
•	Proceeds to execute the Glue job upon successful cataloging.
•	Triggers the target crawler after job completion.
•	For this POC, the workflow is triggered via a scheduled time-based trigger (e.g., daily). However, this can be easily modified for event-driven execution, such as triggering on file upload using S3 event notifications and Lambda.
# 6.	Script Management via S3
An additional S3 bucket is provisioned to store the Glue ETL scripts, keeping them organized and decoupled from the main data buckets.
# 7.	Archival Logic
To keep the source folder clean and avoid reprocessing the same files, a basic archival mechanism is implemented:
•	After the ETL job successfully processes the input file, the original file is moved to an archive/ folder inside the source bucket.
•	This ensures traceability while supporting new incremental uploads.
# Infrastructure Components (via Terraform)
This project provisions the following AWS infrastructure components using Terraform:
# •	S3 Buckets:
o	Source Bucket:
	Contains the raw input CSV files.
	Hosts an archive/ folder for processed input files.
	Stores Athena query results in a dedicated athena/ folder, used internally by the Glue job to read source data.
o	Target Bucket: Stores the processed output files (CSV or Parquet format).
o	Scripts Bucket: Used to centrally store the Glue job script (.py) that is referenced during job execution.
# •	AWS Glue Components:
o	Glue Database: Logical container for source and target tables generated by crawlers.
o	Glue Crawlers:
	Source Crawler: Scans the source S3 location and creates a Glue table from the raw CSV data.
	Target Crawler: Scans the output folder in the target bucket and creates a Glue table for the processed data.
o	Glue Job:
	Python-based job that executes an Athena query over the raw data, calculates the lag time between user events, and writes the results back to the target S3 bucket.
•	Glue Workflow and Triggers:
o	A Glue Workflow orchestrates the complete ETL process.
o	Three Triggers ensure smooth execution:
1. Scheduled Trigger: Starts the workflow periodically (can later be replaced with an event-based trigger such as S3 upload).
2. On Success Trigger 1: Executes the Glue job after the source crawler finishes and the catalog table is created.
3. On Success Trigger 2: Executes the target crawler after the Glue job completes successfully.
# •	IAM Roles & Policies:
o	Glue-specific IAM roles are created with fine-grained permissions to access S3 buckets, Glue catalog, Athena query execution, and logging.

# Deployment Steps
Follow the steps below to deploy and run the ETL workflow from your local Linux machine:
Configure AWS CLI - aws configure
Initialize Terraform - terraform init
Validate Terraform - terraform validate
Verify the plan - terraform plan
Apply the plan - terraform apply
Trigger the Glue Workflow Manually in case of testing it - 
aws glue start-workflow-run --name your-glue-workflow-name

# Execution Results
For visual reference, detailed screenshots of the ETL workflow execution at each stage are provided in the etl_screenshots.pdf file included with this project.

# Future Enhancements 
•	Support for event-based triggering (e.g., S3 PUT event for file upload).
•	Enhanced error handling and logging using Glue job bookmarks or CloudWatch alerts.
•	Integration of AWS Glue Data Quality for schema validation and data profiling.
•	Cost optimization through improved job sizing, dynamic scaling, and efficient partitioning.




